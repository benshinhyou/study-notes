# Flink快速上手

本文通过 Flink DataSet API 和 DataStream API 分别实现一个WordCount案例，统计文件中单词的出现次数。

## 一. 环境准备

在 IDEA 中搭建一个 Flink 项目的骨架。使用 Java 项目中常见的 Maven来进行依赖管理。

引入如下依赖：

```xml
<properties>
    <flink.version>1.17.0</flink.version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java</artifactId>
        <version>${flink.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients</artifactId>
        <version>${flink.version}</version>
    </dependency>
</dependencies>
```

我们在项目根目录新建一个 `input/hello.txt` 文件：

```txt
hello java
hello python
hello golang
hello javascript
hello scala
```

## 二. 批处理实现

```java
package cn.bigcoder.demo.flink.hello;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.AggregateOperator;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.operators.FlatMapOperator;
import org.apache.flink.api.java.operators.UnsortedGrouping;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.util.Collector;


public class _1WordCountBatchDemo {
    public static void main(String[] args) throws Exception {
        // 1. 创建执行环境
        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        // 2. 从文件读取数据 按行读取(存储的元素就是每行的文本)
        DataSource<String> dataSource = env.readTextFile("input/hello.txt");
        // 3. 转换数据格式
        FlatMapOperator<String, Tuple2<String, Integer>> wordCount = dataSource.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            @Override
            public void flatMap(String s, Collector<Tuple2<String, Integer>> collector) throws Exception {
                String[] words = s.split(" ");
                for (String word : words) {
                    collector.collect(Tuple2.of(word, 1));
                }
            }
        });
        // 4. 按照 word 进行分组，只能采用位置索引或属性名称
        UnsortedGrouping<Tuple2<String, Integer>> wordCountGroup = wordCount.groupBy(0);
        // 5. 分组内聚合统计
        AggregateOperator<Tuple2<String, Integer>> sum = wordCountGroup.sum(1);
        // 6. 打印结果
        sum.print();
    }
}
```

运行程序，控制台会打印出结果：

```txt
(scala,1)
(hello,5)
(golang,1)
(java,1)
(python,1)
(javascript,1)
```

需要注意的是，这种代码的实现方式，是基于 DataSet API 的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上 Flink 本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的 API 来实现。所以从 Flink 1.12 开始，官方推荐的做法是直接使用 DataStream API，在提交任务时通过将执行模式设为 BATCH 来进行批处理：

```shell
bin/flink run -Dexecution.runtime-mode=BATCH BatchWordCount.jar
```

## 三. 流处理实现

对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景。

下面我们就针对不同类型的输入数据源，用具体的代码来实现流处理。

```java
package cn.bigcoder.demo.flink.hello;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;


/**
 * @author: Jindong.Tian
 * @date: 2023-08-27
 **/
public class _2WordCOuntStreamDemo {
    public static void main(String[] args) throws Exception {
        // 创建执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 从控制台读取数据
        DataStream<String> input = env.readTextFile("input/hello.txt");

        // 转换数据流，拆分单词并计数
        DataStream<Tuple2<String, Integer>> wordCounts = input
                .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
                    @Override
                    public void flatMap(String s, Collector<Tuple2<String, Integer>> collector) throws Exception {
                        for (String word : s.split(" ")) {
                            if (word.length() > 0) {
                                collector.collect(new Tuple2<>(word, 1));
                            }
                        }
                    }
                })
                .keyBy(0) // 按单词分组
                .sum(1); // 计算每个单词的频率

        // 打印结果
        wordCounts.print();

        // 执行任务
        env.execute("WordCountJob");
    }
}
```

输出结果：

```txt
8> (javascript,1)
1> (scala,1)
5> (hello,1)
5> (python,1)
13> (golang,1)
5> (hello,2)
5> (hello,3)
5> (hello,4)
5> (hello,5)
3> (java,1)
```

主要观察与批处理程序BatchWordCount的不同：

- 创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment。
- 转换处理之后，得到的数据对象类型不同。
- 分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么。
- 代码末尾需要调用env的execute方法，开始执行任务。